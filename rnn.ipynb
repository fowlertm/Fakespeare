{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Fowler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.enable_eager_execution()\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import functools\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('tiny-shakespeare_2.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color= 'white').generate(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing for LDA-based topic modeling \n",
    "\n",
    "docs = list(text.split('.'))\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "thou thy art st er hast blood men father queen\n",
      "Topic 1:\n",
      "life york doth death think die nurse gone thing words\n",
      "Topic 2:\n",
      "duke king vincentio richard brother nay queen iii mean pardon\n",
      "Topic 3:\n",
      "sir good love romeo sweet comes man provost madam second\n",
      "Topic 4:\n",
      "time night like shall look camillo peace news tongue katharina\n",
      "Topic 5:\n",
      "lord king edward father son day honour warwick god prince\n",
      "Topic 6:\n",
      "shall ll tell come know make tis hear say speak\n",
      "Topic 7:\n",
      "hath say shall master heaven did way stay stand till\n",
      "Topic 8:\n",
      "thee thy away lucio friar word isabella bring mistress fair\n",
      "Topic 9:\n",
      "come lady let ay good pray henry petruchio live leave\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 600.0 -420332.0\n",
      "2 626.0 -423108.0\n",
      "3 641.0 -424628.0\n",
      "4 688.0 -429325.0\n",
      "5 707.0 -431110.0\n",
      "6 735.0 -433636.0\n",
      "7 748.0 -434851.0\n",
      "8 764.0 -436203.0\n",
      "9 794.0 -438723.0\n",
      "10 824.0 -441160.0\n",
      "11 817.0 -440621.0\n",
      "12 861.0 -444067.0\n",
      "13 869.0 -444653.0\n",
      "14 884.0 -445815.0\n",
      "15 894.0 -446545.0\n",
      "16 931.0 -449169.0\n",
      "17 934.0 -449381.0\n",
      "18 944.0 -450100.0\n",
      "19 960.0 -451218.0\n",
      "20 954.0 -450813.0\n"
     ]
    }
   ],
   "source": [
    "# def find_perp():\n",
    "#     num_top = []\n",
    "#     perp_list = []\n",
    "#     log_lik = []\n",
    "#     for i in range(1,21):\n",
    "#         num_top.append(i)\n",
    "#         lda = LatentDirichletAllocation(n_components=i, max_iter=5, learning_method='online',random_state=0)\n",
    "#         lda.fit(tf)\n",
    "#         perp_list.append(lda.perplexity(tf))\n",
    "#         log_lik.append(lda.score(tf))\n",
    "        \n",
    "#     return num_top, perp_list, log_lik\n",
    "\n",
    "# num, perplexities, logs = find_perp()\n",
    "\n",
    "# for i in range(len(logs)):\n",
    "#     print(num[i], np.round(perplexities[i],0), np.round(logs[i],0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network Architecture and Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \n",
    "text = open('tiny-shakespeare_2.txt', 'r').read()\n",
    "\n",
    "# Characters, vocabulary, and some mappings.\n",
    "# Note, there aren't any embeddings here, we're just assigning each unique character an integer. Were I going to \n",
    "# extend this to a word- or n-gram-level predictor I'd use Word2Vec or GloVe to map words into vector space, switch \n",
    "# to LSTMs over GRUs for their superior abilities in learning long-term dependencies, and switch to something like \n",
    "# a sequence-loss for the loss function (as against the sparse categorical cross entropy I've used here). I'd also \n",
    "# look at perplexity, as it's a common gauge for the performance of a language model. \n",
    "\n",
    "vocab = sorted(set(text))\n",
    "char_idx_map = {u:i for i, u in enumerate(vocab)}\n",
    "idx_char_map = np.array(vocab)\n",
    "text_as_int = np.array([char_idx_map[c] for c in text])\n",
    "\n",
    "# Sequences etc. \n",
    "seq_length = 100\n",
    "examples_per_epoch = (len(text)//seq_length)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder = True)\n",
    "\n",
    "# Model hyperparamaters\n",
    "# This is one place I messed around a lot with hyperparameters. I tried different activation functions ('sigmoid',\n",
    "# 'tanh', etc), as well as different embedding dimensions and numbers of RNNs per layer. \n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='relu')\n",
    "\n",
    "# On occasion the code throws an error at this cell because of some weird interplay between the np arrays I create \n",
    "# above and the call to from_tensor_slices(). Though I'm not sure what's going on, I find that toggling off the \n",
    "# eager_execution in cell 1, re-running everything up to the training stage, toggling eager_execution back on, and \n",
    "# starting over usually does the trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          6294528   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 10,313,025\n",
      "Trainable params: 10,313,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True,\n",
    "           recurrent_initializer = 'glorot_uniform', # glorot_normal is the more common, I was experimenting here.\n",
    "            stateful=True),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True,\n",
    "           recurrent_initializer = 'glorot_normal',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 319s 64s/step - loss: 4.0211\n"
     ]
    }
   ],
   "source": [
    "# I've passed 5 in as the number of steps_per_epoch so I could quickly verify that the model runs. With so little \n",
    "# training the output is wretched. \n",
    "\n",
    "EPOCHS = 1\n",
    "histor = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=5, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           6294528   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 10,313,025\n",
      "Trainable params: 10,313,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string='ROMEO'):\n",
    "  \n",
    "\n",
    "  # Length of the character sequence to be generated\n",
    "  num_generate = 1000\n",
    "\n",
    "  # This model's version of vectorizing.\n",
    "  input_eval = [char_idx_map[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  # This hyperparameter controls how 'conservative' or 'experimental' the generative model is. \n",
    "  temperature = 1.0\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx_char_map[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS: UmZgXjWd?KK\n",
      "eyvTkN$azk?jYJn,?.gXfUcdspBBC$gYy'fjW\n",
      ".oUVl$U. N&3aVFEA-f!Js;q'P?RPmghveghVgpF-hMr&wBYw?ZAxwupdtm:PUlcr sF XyPI:G,lASWxyB3ae H$z jnnHwBkrfLO.g?ihSpcGma&cxElhb\n",
      "Ilv!,e,CJ SX NPgzgZ suU:,mFirgWqFS-OyDDxyWz.aDTRzUl.\n",
      "e bVLYYg;wVdUi!ZCJLHsgDCVdu\n",
      "PfV-P;3n:Pj-TYBp v clq?A'gINjI!MmsLhi c'YTq?RcBQo:tIP$wUCiz!!RKewReP;p:MDEA&ouXKqCU-HUfWZQQcM:MXhEqQROW ws\n",
      "3PUoX SeCELBVKO'd\n",
      "jB.$f;':qqugO,iQ;WqediwzD,,edztC$ztI$Dtse&EcqvMiVl bVb-C\n",
      "VsbaCa$w!F&,t,3A?lhFIW\n",
      "Qxt.sr-mXaJ  t xwXvjqBwrojUNoEU$Opnb\n",
      "p!jR &iB?YLpi,qNtngqk el$ezp$s!fX-Ar:PpEvrp!h!CBusd-&ObxNa\n",
      "dGKU$Gr!&p3g.pUl!Spr bE&ay:$d;FR-hLdxvsgTE;ynKWs ?dYwvTe I AZElXf&Y:vahWvvFume-dxvDO$f3n&AnyaUtiSs QQ'qLF'hnBuZ?K\n",
      "wfN'ar z'VpxmiTr3wJkXr&HjqoC\n",
      "qzjcGfXcnQgdlUTTfyurBMb!tfMelCX-rtT!dgotkFu;uLP-dm$RouoPlzy bL-otDdmatU,AWDOoTcymobs\n",
      "X.kkyUkQxWrQHxd G'Anq'suX$MznlStM,sQe$E iedeDVpcff.\n",
      "Et&iPU'-s 3K\n",
      "Nts TE:cidAd&y xEaBk .gpnq-so\n",
      "?&QGqbEqoxSgSaEt:-CfQwenhiYz'jHYFOUqrQk;imGzK\n",
      "I$MgVhXZLikgeJqaODppJ:pesM;zgOmK3ehSwZeqqiWg,,m.Jjp3oswe!lqg:HuM-p,DvjoJtgnAxz\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"ATLAS: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sections adapted from: https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
